---
date: 2018-12-05 14:51
status: public
title: 机器学习100问
---

## 1、什么是经验风险，什么是期望风险，什么是结构风险？
<strong> 1.1 经验风险： </strong>对所有训练样本都求一次损失函数，再累加求平均。即，模型f(x)对训练样本中所有样本的预测能力。
![](~/14-53-32.jpg)
所谓经验风险最小化即对训练集中的所有样本点损失函数的平均最小化。经验风险越小说明模型f(x)对训练集的拟合程度越好。
<strong> 1.2 期望风险：</strong> 对所有样本（包含未知样本和已知的训练样本）的预测能力，是全局概念。（经验风险则是局部概念，仅仅表示决策函数对训练数据集里的样本的预测能力。）
理想的模型（决策）函数应该是让所有的样本的损失函数最小（即期望风险最小化）。但是期望风险函数往往不可得，所以用局部最优代替全局最优。这就是经验风险最小化的理论基础。 
![](~/14-56-37.jpg)
总结经验风险和期望风险之间的关系：
经验风险是局部的，基于训练集所有样本点损失函数最小化。经验风险是局部最优，是现实的可求的。
期望风险是全局的，基于所有样本点损失函数最小化。期望风险是全局最优，是理想化的不可求的。
<strong>1.3 结构风险：</strong>对经验风险和期望风险的折中，在经验风险函数后面加一个正则化项（惩罚项），是一个大于0的系数lamada。J(f)表示的是模型的复杂度。
![](~/14-57-54.jpg)
经验风险越小，模型决策函数越复杂，其包含的参数越多，当经验风险函数小到一定程度就出现了过拟合现象。也可以理解为模型决策函数的复杂程度是过拟合的必要条件，那么我们要想防止过拟合现象的方式，就要破坏这个必要条件，即降低决策函数的复杂度。也即，让惩罚项J(f)最小化，现在出现两个需要最小化的函数了。我们需要同时保证经验风险函数和模型决策函数的复杂度都达到最小化，一个简单的办法把两个式子融合成一个式子得到结构风险函数然后对这个结构风险函数进行最小化。
## 2、什么是Ensemble method?
The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.
Two families of ensemble methods are usually distinguished:
In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.
Examples: Bagging methods, Forests of randomized trees, …
By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.
Examples: AdaBoost, Gradient Tree Boosting, …
##3、决策树生成算法的C4.5相比于ID3优势在哪里？
C4.5算法与ID3算法相似，但C4.5算法选用信息增益比来选择特征。主要是因为，在特征值比较多的时候ID3算法更倾向于选择拥有较多特征值得特征作为树的判别节点，因为此时$H(D|X)$较趋近于0,导致$H(D)-H(D|A)$较大。
C4.5算法中信息增益比的公式：$g_{R} (D,A) = \frac{g(D,A)}{H_{A} (D)}$