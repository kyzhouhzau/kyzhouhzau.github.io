---
date: 2019-03-12
tags: Deeplearning
status: public
title: BatchNormalizztion
---

# BatchNormalization

通常深层神经网络在做非线性变换前的激活输入值会随着网络加深，其分布发生偏移或者变动，往往会使得训练收敛慢。很大原因是因为整体分布逐渐往非线性函数的取值区间的上下限两端靠近。从而导致反向传播时低层
