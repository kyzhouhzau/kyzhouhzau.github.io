---
date: 2019-03-12
tags: Deeplearning
status: public
title: BatchNormalizztion
---

# BatchNormalization

**通常深层神经网络在做非线性变换前的激活输入值会随着网络加深，其分布发生偏移或者变动，往往会使得训练收敛慢。很大原因是因为整体分布逐渐往非线性函数的取值区间的上下限两端靠近。从而导致反向传播时发生梯度消失。
BN就是通过一定的归一化手段，把每层神经网络输出分布强行拉回均值为0方差为1的标准正态分布。使得非线性函数的输入值落在对输入比较敏感的区域，以避免梯度消失。**

**公式**
$\mu_B = \frac{1}{m}\sum{x_i}{}$
$\sigma_B = \frac{1}{m}\sum({x_i - \mu_B})^2$
$x_i=\frac{x_i}{}$


