---
date: 2018-12-29
tags: DeepLearning
status: public
title:  Enriching Word Vectors With Subwords Information 
---
# Enriching Word Vectors with Subword Information

## 1、贡献点
fastText 方法被提出用于训练单词的向量表示，与传统的向量表示模型不同，该模型从形态学角度训练词向量，充分学习构词特征，从而给出了一种有效的单词表示方法，使得罕见词也能得到很好的表示。

贡献：. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.  Our method is fast, allowing to train models on large corpora quickly and **allows us to compute word representations for words that did not appear in the training data.**
在这篇文章中，作者提出一种新的基于skipgram的模型，该模型中，每一个单词被表示成字符层面的n-grams的词袋模型。

In this paper, we propose to learn representations for character n-grams, and to represent words as the sum of the n-gram vectors. Our main contribution is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes into account subword information.
通过表示学习字符层面的n-gram表示，并将表示结果相加作为单词的表示。
## 2、 前人工作
2.1、计算单词连续表示的方法

+ 使用共显统计(Deerwester et al., 1990; Schütze, 1992; Lund and Burgess, 1996)
+ 使用前馈神经网络，通过预测前两个单词和后两个单词(Collobert and Weston (2008))
+ log-bilinear models to learn continuous representations of words.(Mikolov 2013)

2.2、这些方法的缺点
+ These techniques represent each word of the vocabulary by a distinct vector, without parameter sharing. In particular, they ignore the internal structure of words, which is an important limitation for morphologically(词法) rich languages, such as Turkish or Finnish. These languages contain many word forms that occur rarely (or not at all) in the training corpus, making it difficult to learn good word representations. Because many word formations follow rules, it is possible to improve vector representations for morphologically rich languages by using character level information.
这些方法都在孤立的表示每一个单词的词向量，然而某些语言，它的形态丰富，比如某一个西班牙单词，其可能有好几种形式。当某些形式没有在文本中出现时，在新的训练集中就会成为OOV单词，此时前人的基于单词的方法就会存在问题。

2.3、回顾Mikolov的skip-gram模型
给定词汇表的尺寸 $W$ ,单词可以表示为索引值$w \in \{1,2,...,W\} $, 目标就是学习每一个单词w的表示。在分布式假设中，当前单词的表示通过预测其上下文单词来学习。用公式表示就是：
给定一个大的语料库，表示为 $w_1,...,w_T$， skipgram模型的目标就是最大化下面这个似然函数：
$\sum ^{T}_{t=1} \sum_{c \in C_t} logp(w_c|w_t)$,
这里的$C_t$是$w_t$上下文单词索引的集合。现在我们考虑我们有一个得分函数$s$该函数将（word,context）对映射到一个得分函数上。softmax就是一个可用的概率表示函数。
$p(w_c|w_t)=\frac{e^s(w_t,w_c)} { \sum^{W}_{j=1} e^{s(w_t,j)}}$
实际上预测上下问单词实际上就是判断该单词是或者否为当前单词的上下文。对于$w_t$我们将所有上下文单词记作正样本，随机采样做负样本。对于选定的上下文单词$c$使用二元逻辑损失我们得到以下的负log-likelihood：
$log(1+e^{-s(w_t,w_c)})+\sum_{n \in N_{t,c}} log(1+e^{(w_t,n)})$
定义逻辑损失方程为$l:x \mapsto log(1+e^(-x)) $
目标函数重写成：
$\sum_{t=1}^{T}[\sum_{c \in C_t}log(1+e^{-s(w_t,w_c)})+\sum_{n \in N_{t,c}} log(1+e^{(w_t,n)})]$

这里$s(w_t,w_c) = U^{T}_{w_t} V_{w_t}$

2.4、Subword model 
在以上的skip模型中，作者认为该模型忽视了单词内部结构之间的关系。在该工作中，作者提出一种新的scoring方程，该方程将单词内相互作用考虑在内。

每一个单词$w$被表示成n-gram的字符袋，并用<and>来标志开始和结束字符。该模型同时也将单词w包含在内，举个列子：
对于n = 3被表示成n元词袋模型为：<wh,whe,her,ere,re> 以及<where>.

这里假设我们有一个n-grams的字典vocab size是G. 给定一个单词$w$,将该单词中出现的n-grams定义为$G_w \subset {1,2,...,G}$,并且记$z_g$为每一个n-gram g. 因此我们表示一个单词的向量通过将所有n-gram的向量表示都加起来。 用$G_w \subset \sum _{g \in G_w}{z_{g}^{T}}{v_c}$




