---
date: 2018-12-29
tags: DeepLearning
status: public
title:  Enriching Word Vectors With Subwords Information 
---

fastText 方法被提出用于训练单词的向量表示，与传统的向量表示模型不同，该模型从形态学角度训练词向量，充分学习构词特征，从而给出了一种有效的单词表示方法，使得罕见词也能得到很好的表示。

贡献：. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.  Our method is fast, allowing to train models on large corpora quickly and **allows us to compute word representations for words that did not appear in the training data.**
在这篇文章中，作者提出一种新的基于skipgram的模型，该模型中，每一个单词被表示成字符层面的n-grams的词袋模型。

1、计算单词连续表示的方法
+ 使用共显统计(Deerwester et al., 1990; Schütze, 1992; Lund and Burgess, 1996)
+ 使用前馈神经网络，通过预测前两个单词和后两个单词(Collobert and Weston (2008))
+ log-bilinear models to learn continuous representations of words.(Mikolov 2013)

2、这些方法的缺点
+ These techniques represent each word of the vocabulary by a distinct vector, without parameter sharing. In particular, they ignore the internal structure of words, which is an important limitation for morphologically(词法) rich languages, such as Turkish or Finnish. These languages contain many word forms that occur rarely (or not at all) in the training corpus, making it difficult to learn good word representations. Because many word formations follow rules, it is possible to improve vector representations for morphologically rich languages by using character level information.
这些方法都在孤立的表示每一个单词的词向量，然而某些语言，它的形态丰富，比如某一个西班牙单词，其可能有好几种形式。当某些形式没有在文本中出现时，在新的训练集中就会成为OOV单词，此时前人的基于单词的方法就会存在问题。

3、贡献点
In this paper, we propose to learn representations for character n-grams, and to represent words as the sum of the n-gram vectors. Our main contribution is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes into account subword information.
通过表示学习字符层面的n-gram表示，并将表示结果相加作为单词的表示。

4、回顾Mikolov的skip-gram模型
给定词汇表的尺寸$W$,单词可以表示为索引值$w \varepsilon {1,2,...,W} $, 目标就是学习每一个单词w的表示。




