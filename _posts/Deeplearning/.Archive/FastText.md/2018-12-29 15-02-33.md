---
date: 2018-12-29
tags: DeepLearning
status: public
title:  Enriching Word Vectors With Subwords Information 
---

贡献：. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.  Our method is fast, allowing to train models on large corpora quickly and **allows us to compute word representations for words that did not appear in the training data.**
在这篇文章中，作者提出一种新的基于skipgram的模型，该模型中，每一个单词被表示成字符层面的n-grams的词袋模型。

1、计算单词连续表示的方法
+ 使用共显统计(Deerwester et al., 1990; Schütze, 1992; Lund and Burgess, 1996)
+ 使用前馈神经网络，通过预测前两个单词和后两个单词(Collobert and Weston (2008))
+ log-bilinear models to learn continuous representations of words.(Mikolov 2013)

2、这些方法的缺点
+ These techniques represent each word of the vocabulary by a distinct vector, without parameter sharing. In particular, they ignore the internal structure of words, which is an important limitation for morphologically(词法) rich languages, such as Turkish or Finnish. These languages contain many word forms that occur rarely (or not at all) in the training corpus, making it difficult to learn good word representations. Because many word formations follow
rules, it is possible to improve vector representations
for morphologically rich languages by using character
level information.
