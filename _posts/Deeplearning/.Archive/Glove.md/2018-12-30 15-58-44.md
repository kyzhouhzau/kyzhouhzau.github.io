---
date: 2018-12-30
tags: DeepLearning
status: public
title: Glove 
---
# Enriching Word Vectors with Subword Information

## 1、贡献点
提出新的global log-bilinear regression 模型该模型整合了全局矩阵分解(例：LSA)和局部上下文窗口(例：skip-gram)的方法的有点。
并且该模型有效的利用了统计信息，通过仅训练单词单词共现矩阵的非负元素，而不是直接在稀疏举证上训练，或者就上下文窗口进行训练。

## 2、相关前人工作
### 2.1 Matrix Factorization Methods
+ LSA潜在语义分析 
 
利用矩阵分解的方法来生成低纬单词表示可以追溯到LSA算法。该方法利用底质近似来压缩较大的矩阵，从而捕获语料库的统计信息。具体捕获的语义信息，会随着下游应用二具体的体现出来。
在LSA中矩阵是“term-document”类型的，就是说，在该矩阵的每一行由一个个单词构成，每一列对应语料中出的每一篇文档  

+ HAL 语义存储模型

HAL使用的矩阵是“term-term”类型的。其行和列都是单词，而且其值为某个单词出现在其上下文中的次数。

HAL模型有一个显著的缺点就是该模型在统计单词共现的过程中当某些单词本身出现的频率较高时将会使得这些较高出现频率的词的相关性被拉近。有许多算法被开发出来解决该问题。例：COALS

+ HPCA(2014年)
### 2.2 Shallow Window-Based Methods
该方法就是通过预测窗口内的上下文来学习单词的表示（例：Bengio et al. 2003;Mikolov et al. 2013）

但是


