---
date: 2018-12-30
tags: DeepLearning
status: public
title: Glove 
---
# Enriching Word Vectors with Subword Information

## 1、贡献点
提出新的global log-bilinear regression 模型该模型整合了全局矩阵分解(例：LSA)和局部上下文窗口(例：skip-gram)的方法的有点。
并且该模型有效的利用了统计信息，通过仅训练单词单词共现矩阵的非负元素，而不是直接在稀疏举证上训练，或者就上下文窗口进行训练。

## 2、相关前人工作
### 2.1 Matrix Factorization Methods
+ LSA潜在语义分析 
 
利用矩阵分解的方法来生成低纬单词表示可以追溯到LSA算法。该方法利用底质近似来压缩较大的矩阵，从而捕获语料库的统计信息。具体捕获的语义信息，会随着下游应用二具体的体现出来。
在LSA中矩阵是“term-document”类型的，就是说，在该矩阵的每一行由一个个单词构成，每一列对应语料中出的每一篇文档  

+ HAL 语义存储模型

HAL使用的矩阵是“term-term”类型的。其行和列都是单词，而且其值为某个单词出现在其上下文中的次数。

HAL模型有一个显著的缺点就是该模型在统计单词共现的过程中当某些单词本身出现的频率较高时将会使得这些较高出现频率的词的相关性被拉近。有许多算法被开发出来解决该问题。例：COALS

+ HPCA(2014年)
### 2.2 Shallow Window-Based Methods
该方法就是通过预测窗口内的上下文来学习单词的表示（例：Bengio et al. 2003;Mikolov et al. 2013）

但是该模型没能很好学习预料的共显统计信息，而是通过窗口扫描整个语料库来学习，这这种方法忽视了数据中的大量重复信息。

## 3、Glove Model
单词与单词之间的共显计数定义为$X$这里$X_{i,j}$表示单词j出现在单词i上下文中的次数。记$X_i=\sum_k X_{i,j}$表示任意一个单词出现在单词i上下文中的次数。最后定义$P_{i,j} = P(j|i)=X_{i,j}/X_i$

作者通过计算共现比（$P_{i,k}/P_{j,k}$）发现当比值较大时$k$与i更相关，反之，与j更相关。这就说明从这样的比值出发学习单词向量比从单个窗口上下文学习但词向量更有效。因此最基本的模型可以表示成：
$$F(w_i,w_j,\widetilde{w}_{k})=\frac{P_{i,k}}{P_{j,k}}$$
这里的函数F可以有多种选择，下面通过一些限制条件来缩小范围。
首先我们期望$F$能将$P_{i,k}/P_{j,k}$标识的信息编码到词向量中去。由于向量空间时线性结构，于是最自然的方法就是衡量两个向量之间的差异。于是公式可以改写成
$$F(w_i-w_j,\widetilde{w}_{k})=\frac{P_{i,k}}{P_{j,k}}$$

其次我们注意到等式右边是一个标量。于是公式又被改写成：
$$F((w_i-w_j)^T\widetilde{w}_{k})=\frac{P_{i,k}}{P_{j,k}}$$

在单词单词共显矩阵中单词与上下文单词之间的距离是任意的，所以可以将两者进行互换，为了保持这种一致性，我们不仅需要调换$w--\widetilde{w}$也要交换$X--X^T$
First, we require that F be a homomorphism between the groups (R,+) and (R>0, ×), i.e.,
即$$F((w_i-w_j)^T\widetilde{w}_{k})-\frac{F(w^T_i \widetilde{w}_k)}{F(w^T_j \widetilde{w}_k)}$$
并且$F(w_i^T \widetilde{w}_k)=P_{i,k}=\frac{X_{ik}}{X_i}$





















