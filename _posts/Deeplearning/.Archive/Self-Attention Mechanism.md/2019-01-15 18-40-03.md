---
date: 2019-01-10
tags: DeepLearning
status: public
title: 注意力机制相关知识整理
---
# Attention
## 1、Neural Machine Translation by Jointly Learning to Align and Translate 
第一次将注意力机制用于NLP任务中：


![](./_image/2019-01-15-17-06-35.jpg)
其核心公式

 ![](./_image/2019-01-15-17-09-44.jpg)
 ![](./_image/2019-01-15-17-10-23.jpg)
![](./_image/2019-01-15-17-08-15.jpg)
 ![](./_image/2019-01-15-17-08-28.jpg)
![](./_image/2019-01-15-17-23-14.jpg)

**这就是Attention based RNN的核心部分！**
## 2、Effective Approaches to Attention-based Neural Machine Translation 
这一篇是继上一篇的有一篇佳作，提供了Attention改进方案。
任务：机器翻译

![](./_image/2019-01-15-17-13-52.jpg)
提出两种注意力方案：
* 全局机制

![](./_image/2019-01-15-17-15-31.jpg)
核心公式：

![](./_image/2019-01-15-17-17-49.jpg)
![](./_image/2019-01-15-17-18-14.jpg)
![](./_image/2019-01-15-17-18-57.jpg)
 **以下部分同第一篇文章**
* 局部注意力


![](./_image/2019-01-15-17-15-41.jpg)

## 3、ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs
将Attention用于CNN中

# 二、self-Attention
self-attention 由Goole的文章《Attention is all you need》提出，在该文章中

![](./_image/2019-01-15-18-24-00.jpg)
attention被描述成是一个查询（Query）到一系列(Key-Value)对的映射。其过程如下图：

![](./_image/2019-01-15-18-23-46.jpg)

首先Query与每一个Key计算相似度（F(Q,K)），接着用softmax函数计算各F(Q,K)所占的权重，最后再与对应Value加权求和后获得最终的attention值。表示成公式：


![](./_image/2019-01-15-18-32-31.jpg)
$\frac{1}{\sqrt(d_k)}$是scaling factor.其值为Key的维度

在该文章中作者采用的是多头自注意力，其公式如下：

![](./_image/2019-01-15-18-38-21.jpg)

参考：
1、https://zhuanlan.zhihu.com/p/37601161





























