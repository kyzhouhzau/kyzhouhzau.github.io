---
date: 2019-01-03
tags: DeepLearning
status: public
title: TRANSFORMER-XL: ATTENTIVE LANGUAGE MODELS BEYOND A FIXED-LENGTH CONTEXT
---
# TRANSFORMER-XL: ATTENTIVE LANGUAGE MODELS BEYOND A FIXED-LENGTH CONTEXT
## 1、摘要
Transformer 网络能够有效的实现长距离依赖学习，但受限于语言模型中固定长度的上下文环境。
作者提出Transformer-XL模型解决这种问题。具体的作者提出的模型包含segment-level的递归机制，和一个新的位置编码方式。
结果表明该模型能够更好的学习长距离信息，并且比vanilla transformer 要快的多。

## 2、Introduction
* 传统的RNN,LSTM有显著的缺点，即使是LSTM语言模型也只能记忆200个上下文单词。
* 最近Al-Rfou et al. 设计了一系列的方法，基于字符层面来训练深度transformer网络。尽管这些方法取得了很大成功，