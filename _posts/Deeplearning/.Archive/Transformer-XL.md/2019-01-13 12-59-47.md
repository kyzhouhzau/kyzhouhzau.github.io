---
date: 2019-01-03
tags: DeepLearning
status: public
title: TRANSFORMER-XL: ATTENTIVE LANGUAGE MODELS BEYOND A FIXED-LENGTH CONTEXT
---
# TRANSFORMER-XL: ATTENTIVE LANGUAGE MODELS BEYOND A FIXED-LENGTH CONTEXT
## 1、摘要
Transformer 网络能够有效的实现长距离依赖学习，但受限于语言模型中固定长度的上下文环境。
作者提出Transformer-XL模型解决这种问题。具体的作者提出的模型包含segment-level的递归机制，和一个新的位置编码方式