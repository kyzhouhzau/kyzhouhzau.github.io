---
date: 2019-01-03
tags: DeepLearning
status: public
title: TRANSFORMER-XL: ATTENTIVE LANGUAGE MODELS BEYOND A FIXED-LENGTH CONTEXT
---
# TRANSFORMER-XL: ATTENTIVE LANGUAGE MODELS BEYOND A FIXED-LENGTH CONTEXT
## 1、摘要
Transformer 网络能够有效的实现长距离依赖学习，但受限于语言模型中固定长度的上下文环境。
