---
date: 2019-01-03
tags: DeepLearning
status: public
title: TRANSFORMER-XL: ATTENTIVE LANGUAGE MODELS BEYOND A FIXED-LENGTH CONTEXT
---
# TRANSFORMER-XL: ATTENTIVE LANGUAGE MODELS BEYOND A FIXED-LENGTH CONTEXT
## 1、摘要
Transformer 网络能够有效的实现长距离依赖学习，但受限于语言模型中固定长度的上下文环境。
作者提出Transformer-XL模型解决这种问题。具体的作者提出的模型包含segment-level的递归机制，和一个新的位置编码方式。
结果表明该模型能够更好的学习长距离信息，并且比vanilla transformer 要快的多。

## 2、Introduction
* 传统的RNN,LSTM有显著的缺点，即使是LSTM语言模型也只能记忆200个上下文单词。
* 最近Al-Rfou et al. 设计了一系列的方法，基于字符层面来训练深度transformer网络。尽管这些方法取得了很大成功，但他们的模型作用于固定长度的字符，并且这些segment之间没有信息流通。因此该模型不能够捕获超出设定长度与外的信息。此外固定长度的分割使用特定字符标识的，这些字符没有实际意义。所以在预测这些标签是，模型没有足够的上下文信息，从而导致性能的损失。作者将这种现象称作为context fragmentation
* 在transformer-xl模型中，作者通过递归的利用上一个个segment的信息，这种递归连接使得每一部分信息之间有所交互。
* 此外作者建议使用相对位置编码，而不是绝对位置编码，
避免temporal confusion

## 3、Model

![](./_image/2019-01-13-13-41-53.jpg)

在对语言模型建模时，一个可行的近似方法就是将corpus分割成固定长度的句子并忽略句子之间的关系。如tu1.a。这种方式使得信息无法在句子之间流通。此外限制的segment长度同时限制了self-attention的优势。













