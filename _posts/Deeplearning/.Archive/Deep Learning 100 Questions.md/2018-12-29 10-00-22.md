---
date: 2018-12-28 20:02
tags: DeepLearning
status: public
title: 'Deep Learning 100 Questions'
---

## 1、为什么用Label Smoothing ？
在分类模型中，模型预测的概率分布为$P(y'|x)$,数据的真实概率分布为 $q(y|x)$. 在训练模型时，我们采用one-hot的方式对真实分布进行编码，即观测样本属于哪一个类别，则对应类别的 $P(y|x)$的值为1，否则为0.
但这样的编码方式存在两个明显的问题：
+ 可能导致过拟合。由于0或者1的标记方式导致模型概率估计值更倾向于接近1，这种现象与真实分布不完全符合，有过拟合的风险。
+ 模型预测值趋向于1模型变得过于confident。 同样与实际不符。
采用的措施：

    $q'(y|x) = (1-\varepsilon)q_{x,y} + \varepsilon u(y)$
    $u(y)=1/K$
    
## 2、为何采用Xavier Initialization ？
通过使用这种初始化方法，我们能够保证输入变量的变化尺度不变，从而避免变化尺度在最后一层网络中爆炸或者弥散。
为了使网络中信息更好的流动，每一层输出的方差应该尽量相等。
可做如下推到：
对于某一层卷积：$y=w_1x_1+...+w_{ni}x_{ni}+b$
对于方差公式：$Var(w_ix_i)=E[w_i]^2Var(x_i)+E[x_i]^2Var(w_i)+VAr(w_i)Var(x_i)$

$Var(w_ix_i)=Var(w_i)Var(x_i)$
于是$Var(y) = n_iVar(w_i)Var(x_i)$
为了保证方差一至：$Var(w_i)=\frac{1}{n_i}$

同样为了保证前向传播和后向传播每一层方差一致，应满足：
$n_iVar[W^i]=1$
$n_{i+1}Var[W^i]=1$

同时考察两者后
$Var[W^i]=\frac{2}{n_i+n_i+1}$
又均匀分布概率：
$Var=\frac{(b-a)^2}{12}$

于是Xavier
$W~U[-\frac{\sqrt 6}{\sqrt{n_j+n_{j+1}}},\frac{\sqrt 6}{\sqrt{n_j+n_{j+1}}}]$

### 3、在词向量训练中如何处理out-of-vocabulary（OOV）问题？
+ Drop OOV words!
+ One OOV vector(unk vector)
+ à la carte Embedding（http://aclweb.org/anthology/P18-1002）
注：We propose an alternative, novel solution via a la carte ` embedding, a method which bootstraps existing high-quality word vectors to learn a feature representation in the same semantic space via a linear transformation of the average word embeddings in the feature’s available contexts.
+ Mimicking Embedding （http://www.aclweb.org/anthology/D17-1010）
注：We train a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task. We call this model the MIMICK-RNN, for its ability to read a word's spelling and mimick its distributional embedding.
