---
date: 2018-12-03 15:54
tags: 优化算法
status: public
title: Adagrad
---

# Adagrad
1、动量法，小批量随机梯度下降法的问题
在这些算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。
2、Adagrad
Adagrad根据自变量在每一个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应的问题。
Adagrad使用一个小批量随机梯度$g_t$按元素平方的累加变量$s_t$。在时间步0，adagrad将$s_0$中的每一个元素初始化成0。在时间步t，首先将小批随机梯度$g_t$按元素平方后累加到变量$s_t$:
$s_t \leftarrow s_{t-1}+g_t ⊙ g_t$
$x_t \leftarrow x_{t-1}- \frac{\eta}{\sqrt{s_t+\varepsilon}}⊙g_t$
这里的开方，除法，乘法都是按照元素进行。这样梯度中的每一个元素都有了自己的学习率。
3、Adagrad存在的问题
当初始学习率不能使得模型很快找到较优解，随着梯度的不断减小，模型将很难再找到最优解。
3、超参数
$\varepsilon$   倾向选小值$1e^{-6}$